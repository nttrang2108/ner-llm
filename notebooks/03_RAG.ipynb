{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n",
        "\n",
        "### 1.1 Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from loguru import logger\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Import evaluation functions\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "from src.utils.evaluation import (\n",
        "    calculate_accuracy,\n",
        "    print_comparison_table,\n",
        "    parse_ner_response\n",
        ")\n",
        "\n",
        "\n",
        "# Suppress verbose logs from httpx and chromadb\n",
        "import logging\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.WARNING)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:18.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m  Configuration:\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:18.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m{\n",
            "  \"ollama_host\": \"http://127.0.0.1:11434\",\n",
            "  \"embedding_model\": \"nomic-embed-text:latest\",\n",
            "  \"llm_model\": \"mistral:7b\",\n",
            "  \"chroma_persist_dir\": \"./chroma_db\",\n",
            "  \"chroma_collection\": \"ner_kb\",\n",
            "  \"chunk_size\": 1500,\n",
            "  \"chunk_overlap\": 100,\n",
            "  \"min_chunk_size\": 50,\n",
            "  \"top_k_retrieval\": 5,\n",
            "  \"retrieval_threshold\": 0.5,\n",
            "  \"temperature\": 0.1,\n",
            "  \"max_tokens\": 1024,\n",
            "  \"top_p\": 0.9,\n",
            "  \"top_k\": 40\n",
            "}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Ollama configuration\n",
        "    \"ollama_host\": os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"),\n",
        "    \"embedding_model\": os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-text:latest\"),\n",
        "    \"llm_model\": os.getenv(\"LLM_MODEL\", \"mistral:7b\"),  # Using smaller model for stability\n",
        "    \n",
        "    # Chroma configuration\n",
        "    \"chroma_persist_dir\": os.getenv(\"CHROMA_PERSIST_DIR\", \"./chroma_db\"),\n",
        "    \"chroma_collection\": os.getenv(\"CHROMA_COLLECTION\", \"ner_kb\"),\n",
        "    \n",
        "    # Chunking configuration\n",
        "    \"chunk_size\": int(os.getenv(\"CHUNK_SIZE\", \"1500\")),  # 800 chars = safe for embedding\n",
        "    \"chunk_overlap\": int(os.getenv(\"CHUNK_OVERLAP\", \"100\")),  # More overlap preserves context\n",
        "    \"min_chunk_size\": int(os.getenv(\"MIN_CHUNK_SIZE\", \"50\")),  # Minimum useful chunk\n",
        "    \n",
        "    # Retrieval configuration\n",
        "    \"top_k_retrieval\": int(os.getenv(\"TOP_K_RETRIEVAL\", \"5\")),  # Reduced for smaller KB\n",
        "    \"retrieval_threshold\": float(os.getenv(\"RETRIEVAL_THRESHOLD\", \"0.5\")),\n",
        "    \n",
        "    # LLM inference configuration\n",
        "    \"temperature\": float(os.getenv(\"TEMPERATURE\", \"0.1\")),\n",
        "    \"max_tokens\": int(os.getenv(\"MAX_TOKENS\", \"1024\")),\n",
        "    \"top_p\": float(os.getenv(\"TOP_P\", \"0.9\")),\n",
        "    \"top_k\": int(os.getenv(\"TOP_K\", \"40\")),\n",
        "}\n",
        "\n",
        "logger.info(\"  Configuration:\")\n",
        "logger.info(json.dumps(CONFIG, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document Chunking Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define `VietnameseDocumentChunker` class to chunk Vietnamese text into smaller chunks with metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from underthesea import sent_tokenize\n",
        "\n",
        "class VietnameseDocumentChunker:\n",
        "    \"\"\"Production-grade Vietnamese document chunker using underthesea\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 chunk_size: int = 1500,\n",
        "                 overlap: int = 100,\n",
        "                 min_chunk_size: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "    \n",
        "    def chunk_by_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text by Vietnamese sentences using underthesea\"\"\"\n",
        "        try:\n",
        "            # Vietnamese sentence segmentation\n",
        "            sentences = sent_tokenize(text)\n",
        "            return [s.strip() for s in sentences if s.strip()]\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Underthesea failed, falling back to regex: {e}\")\n",
        "            # Fallback to regex-based splitting\n",
        "            sentence_markers = r'([.!?]|â€¦|\\n\\n)'\n",
        "            parts = re.split(sentence_markers, text)\n",
        "            sentences = []\n",
        "            for i in range(0, len(parts)-1, 2):\n",
        "                sent = parts[i] + (parts[i+1] if i+1 < len(parts) else \"\")\n",
        "                if sent.strip():\n",
        "                    sentences.append(sent.strip())\n",
        "            return sentences\n",
        "    \n",
        "    def chunk_with_metadata(self, \n",
        "                           text: str, \n",
        "                           doc_id: str,\n",
        "                           metadata: Dict = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Chunk text with metadata and context overlap\"\"\"\n",
        "        \n",
        "        sentences = self.chunk_by_sentences(text)\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        \n",
        "        for sent in sentences:\n",
        "            sent_len = len(sent)  # CHARACTER count (FIXED: was word count!)\n",
        "            \n",
        "            if current_length + sent_len <= self.chunk_size:\n",
        "                current_chunk.append(sent)\n",
        "                current_length += sent_len\n",
        "            else:\n",
        "                # Save current chunk if large enough\n",
        "                if current_length >= self.min_chunk_size:\n",
        "                    chunk_text = \" \".join(current_chunk)\n",
        "                    chunks.append({\n",
        "                        \"text\": chunk_text,\n",
        "                        \"doc_id\": doc_id,\n",
        "                        \"chunk_index\": len(chunks),\n",
        "                        \"char_count\": current_length,  # Changed from word_count\n",
        "                        \"metadata\": metadata or {}\n",
        "                    })\n",
        "                \n",
        "                # Start new chunk with overlap (last 2 sentences)\n",
        "                overlap_sents = current_chunk[-2:] if len(current_chunk) >= 2 else current_chunk[-1:]\n",
        "                current_chunk = overlap_sents + [sent]\n",
        "                current_length = sum(len(s) for s in current_chunk)  # CHARACTER count\n",
        "        \n",
        "        # Save last chunk\n",
        "        if current_chunk and current_length >= self.min_chunk_size:\n",
        "            chunks.append({\n",
        "                \"text\": \" \".join(current_chunk),\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_index\": len(chunks),\n",
        "                \"char_count\": current_length,  # Changed from word_count\n",
        "                \"metadata\": metadata or {}\n",
        "            })\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "# Initialize chunker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a chunker instance with the configured parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.702\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[32m\u001b[1mChunker initialized with parameters:\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1m  Chunk size: 1500\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1m  Overlap: 100\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1m  Min chunk size: 50\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "chunker = VietnameseDocumentChunker(\n",
        "    chunk_size=CONFIG[\"chunk_size\"], \n",
        "    overlap=CONFIG[\"chunk_overlap\"],\n",
        "    min_chunk_size=CONFIG[\"min_chunk_size\"]\n",
        ")\n",
        "logger.success(\"Chunker initialized with parameters:\")\n",
        "logger.info(f\"  Chunk size: {CONFIG['chunk_size']}\")\n",
        "logger.info(f\"  Overlap: {CONFIG['chunk_overlap']}\")\n",
        "logger.info(f\"  Min chunk size: {CONFIG['min_chunk_size']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedding Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.728\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[32m\u001b[1mEmbedding model initialized: nomic-embed-text:latest\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m   Vector dimension: 768\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Initialize embedder\n",
        "embedder = OllamaEmbeddings(\n",
        "    model=CONFIG[\"embedding_model\"],\n",
        "    base_url=CONFIG[\"ollama_host\"]\n",
        ")\n",
        "\n",
        "# Test embedding\n",
        "try:\n",
        "    test_embedding = embedder.embed_query(\"Kiểm tra\")\n",
        "    logger.success(f\"Embedding model initialized: {CONFIG['embedding_model']}\")\n",
        "    logger.info(f\"   Vector dimension: {len(test_embedding)}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to initialize embedding model: {e}\")\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vector Store (Chroma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.888\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[32m\u001b[1m“ Chroma VectorStore initialized\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m  Collection: ner_kb\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m  Persist directory: chroma_db\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Initialize Chroma vector store\n",
        "try:\n",
        "    # Create persist directory if it doesn't exist\n",
        "    persist_dir = Path(CONFIG[\"chroma_persist_dir\"])\n",
        "    persist_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    vector_store = Chroma(\n",
        "        collection_name=CONFIG[\"chroma_collection\"],\n",
        "        embedding_function=embedder,\n",
        "        persist_directory=str(persist_dir)\n",
        "    )\n",
        "    \n",
        "    # Get collection stats\n",
        "    collection = vector_store._collection\n",
        "    count = collection.count()\n",
        "    \n",
        "    logger.success(\"“ Chroma VectorStore initialized\")\n",
        "    logger.info(f\"  Collection: {CONFIG['chroma_collection']}\")\n",
        "    logger.info(f\"  Persist directory: {persist_dir}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"— Chroma initialization error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prompt Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGPromptBuilder:\n",
        "    \"\"\"Build context-aware RAG prompts for NER\"\"\"\n",
        "    \n",
        "    SYSTEM_PROMPT = \"\"\"You are an expert Vietnamese Named Entity Recognition (NER) system.\n",
        "\n",
        "Task: Extract named entities from Vietnamese text into three categories.\n",
        "\n",
        "Entity Categories:\n",
        "1. person: Names of people, individuals\n",
        "2. organizations: Company names, institutions, government agencies\n",
        "3. address: Geographic locations, addresses, place names\n",
        "\n",
        "Requirements:\n",
        "1. Extract ALL entity mentions (even if they appear multiple times)\n",
        "2. Preserve exact original Vietnamese text (no normalization)\n",
        "3. Include titles and descriptors when they're part of the entity\n",
        "4. Return ONLY valid JSON output\n",
        "\n",
        "Output Format:\n",
        "{\n",
        "    \"person\": [\"Entity 1\", \"Entity 2\"],\n",
        "    \"organizations\": [\"Entity 1\", \"Entity 2\"],\n",
        "    \"address\": [\"Entity 1\", \"Entity 2\"]\n",
        "}\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def build_rag_prompt(input_text: str, \n",
        "                        retrieved_docs: List[Document],\n",
        "                        top_k: int = 5) -> str:\n",
        "        \"\"\"Build RAG prompt with similar examples\"\"\"\n",
        "        \n",
        "        # Format retrieved examples\n",
        "        examples_section = \"### Similar Examples from Knowledge Base:\\n\"\n",
        "        for i, doc in enumerate(retrieved_docs[:top_k], 1):\n",
        "            # text_preview = doc.page_content[:300]\n",
        "            # examples_section += f\"\\nExample {i}:\\n\"\n",
        "            # examples_section += f\"Text: {text_preview}\\n\"\n",
        "            examples_section += f\"\\nExample {i}:\\n\"\n",
        "            examples_section += f\"Text: {doc.page_content}\\n\"\n",
        "        \n",
        "        if not retrieved_docs:\n",
        "            examples_section = \"### Note: No similar examples found in knowledge base.\\n\"\n",
        "        \n",
        "        # Build full prompt\n",
        "        prompt = f\"\"\"{RAGPromptBuilder.SYSTEM_PROMPT}\n",
        "\n",
        "{examples_section}\n",
        "\n",
        "# ## Input Text:\n",
        "\\\"\\\"\\\"\n",
        "{input_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "# ## Output (JSON only, no explanation):\n",
        "\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "\n",
        "# Test prompt builder\n",
        "test_docs = [\n",
        "    Document(page_content=\"Nguyễn Văn A là CEO của công ty B. Anh sinh năm 1980 tại Hà Nội.\")\n",
        "]\n",
        "test_prompt = RAGPromptBuilder.build_rag_prompt(\n",
        "    input_text=\"Tôi là Phạm Thị C, làm việc tại Đại học X\",\n",
        "    retrieved_docs=test_docs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. NER Extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGNERExtractor:\n",
        "    \"\"\"Production RAG-enhanced NER extractor\"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 embedder: OllamaEmbeddings,\n",
        "                 vector_store: Chroma,\n",
        "                 llm_model: str = \"ministral-3:14b\",\n",
        "                 ollama_host: str = \"http://127.0.0.1:11434\",\n",
        "                 temperature: float = 0.1,\n",
        "                 top_k_retrieval: int = 3):\n",
        "        \n",
        "        self.embedder = embedder\n",
        "        self.vector_store = vector_store\n",
        "        self.top_k_retrieval = top_k_retrieval\n",
        "        \n",
        "        # Initialize LLM\n",
        "        try:\n",
        "            self.llm = OllamaLLM(\n",
        "                base_url=ollama_host,\n",
        "                model=llm_model,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                top_k=40,\n",
        "                num_predict=1024\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize LLM: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _deduplicate_chunks(self, docs: List[Document], max_per_doc: int = 2) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Deduplicate chunks from same document to avoid redundancy.\n",
        "        \n",
        "        Strategy:\n",
        "        - Group chunks by doc_id\n",
        "        - Keep max_per_doc best chunks from each document\n",
        "        - If multiple chunks from same doc are consecutive, merge them\n",
        "        \n",
        "        Args:\n",
        "            docs: Retrieved documents (chunks)\n",
        "            max_per_doc: Maximum chunks to keep from same document\n",
        "        \n",
        "        Returns:\n",
        "            Deduplicated list of documents\n",
        "        \"\"\"\n",
        "        if not docs:\n",
        "            return docs\n",
        "        \n",
        "        # Group chunks by doc_id\n",
        "        from collections import defaultdict\n",
        "        doc_groups = defaultdict(list)\n",
        "        \n",
        "        for idx, doc in enumerate(docs):\n",
        "            doc_id = doc.metadata.get(\"doc_id\", f\"unknown_{idx}\")\n",
        "            doc_groups[doc_id].append((idx, doc))\n",
        "        \n",
        "        # Keep top chunks per document\n",
        "        deduplicated = []\n",
        "        seen_doc_ids = set()\n",
        "        \n",
        "        for doc in docs:\n",
        "            doc_id = doc.metadata.get(\"doc_id\", None)\n",
        "            \n",
        "            if doc_id not in seen_doc_ids:\n",
        "                # First chunk from this document - always keep\n",
        "                deduplicated.append(doc)\n",
        "                seen_doc_ids.add(doc_id)\n",
        "            elif len([d for d in deduplicated if d.metadata.get(\"doc_id\") == doc_id]) < max_per_doc:\n",
        "                # Add more chunks up to limit\n",
        "                deduplicated.append(doc)\n",
        "        \n",
        "        \n",
        "        return deduplicated\n",
        "    \n",
        "    def extract_with_rag(self,\n",
        "                        text: str,\n",
        "                        use_rag: bool = True,\n",
        "                        return_context: bool = False) -> Dict[str, Any]:\n",
        "        \"\"\"Extract entities using RAG\"\"\"\n",
        "        \n",
        "        result = {\n",
        "            \"text\": text,\n",
        "            \"entities\": {\"person\": [], \"organizations\": [], \"address\": []},\n",
        "            \"retrieval_info\": None,\n",
        "            \"error\": None\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Step 1: Retrieve similar chunks\n",
        "            retrieved_docs = []\n",
        "            retrieval_time = 0\n",
        "            \n",
        "            if use_rag:\n",
        "                ret_start = time.time()\n",
        "                try:\n",
        "                    # Similarity search\n",
        "                    # Truncate query to prevent errors\n",
        "                    # Max 1500 chars for embedding\n",
        "                    query_text = text[:1500] if len(text) > 1500 else text\n",
        "                    \n",
        "                    retrieved_docs = self.vector_store.similarity_search(\n",
        "                        query=query_text,\n",
        "                        k=self.top_k_retrieval\n",
        "                    )\n",
        "                    retrieval_time = time.time() - ret_start\n",
        "                    \n",
        "                    # Deduplicate chunks\n",
        "                    retrieved_docs = self._deduplicate_chunks(retrieved_docs, max_per_doc=1)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Retrieval failed, proceeding without RAG: {e}\")\n",
        "                    use_rag = False\n",
        "            \n",
        "            # Step 2: Build prompt\n",
        "            prompt = RAGPromptBuilder.build_rag_prompt(\n",
        "                input_text=text,\n",
        "                retrieved_docs=retrieved_docs,\n",
        "                top_k=self.top_k_retrieval\n",
        "            )\n",
        "            \n",
        "            # Step 3: Call LLM\n",
        "            llm_start = time.time()\n",
        "            response = self.llm.invoke(prompt)\n",
        "            llm_time = time.time() - llm_start\n",
        "            \n",
        "            # Step 4: Parse response\n",
        "            entities = self._parse_response(response)\n",
        "            result[\"entities\"] = entities\n",
        "            \n",
        "            # Step 5: Collect metadata\n",
        "            total_time = time.time() - start_time\n",
        "            result[\"retrieval_info\"] = {\n",
        "                \"num_chunks\": len(retrieved_docs),\n",
        "                \"retrieval_time\": retrieval_time,\n",
        "                \"llm_time\": llm_time,\n",
        "                \"total_time\": total_time,\n",
        "                \"used_rag\": use_rag\n",
        "            }\n",
        "            \n",
        "            if not return_context:\n",
        "                result.pop(\"retrieval_info\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            result[\"error\"] = str(e)\n",
        "            logger.error(f\"Extraction error: {e}\", exc_info=True)\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def _parse_response(response: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Parse LLM JSON response\"\"\"\n",
        "        \n",
        "        import re\n",
        "        \n",
        "        # Extract JSON from response\n",
        "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response, re.DOTALL)\n",
        "        \n",
        "        default_result = {\n",
        "            \"person\": [],\n",
        "            \"organizations\": [],\n",
        "            \"address\": []\n",
        "        }\n",
        "        \n",
        "        if not json_match:\n",
        "            logger.warning(\"No JSON found in LLM response\")\n",
        "            return default_result\n",
        "        \n",
        "        try:\n",
        "            parsed = json.loads(json_match.group())\n",
        "            \n",
        "            result = {\n",
        "                \"person\": parsed.get(\"person\", []),\n",
        "                \"organizations\": parsed.get(\"organizations\", []),\n",
        "                \"address\": parsed.get(\"address\", [])\n",
        "            }\n",
        "            \n",
        "            # Ensure all values are lists\n",
        "            for key in result:\n",
        "                if not isinstance(result[key], list):\n",
        "                    result[key] = [str(result[key])] if result[key] else []\n",
        "            \n",
        "            return result\n",
        "        \n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.warning(f\"JSON parse error: {e}\")\n",
        "            return default_result\n",
        "\n",
        "# Initialize extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.919\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[32m\u001b[1m“ RAGNERExtractor initialized\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    ner_extractor = RAGNERExtractor(\n",
        "        embedder=embedder,\n",
        "        vector_store=vector_store,\n",
        "        llm_model=CONFIG[\"llm_model\"],\n",
        "        ollama_host=CONFIG[\"ollama_host\"],\n",
        "        temperature=CONFIG[\"temperature\"],\n",
        "        top_k_retrieval=CONFIG[\"top_k_retrieval\"]\n",
        "    )\n",
        "    logger.success(\"“ RAGNERExtractor initialized\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"— RAGNERExtractor error: {e}\")\n",
        "    logger.info(f\"  Make sure the LLM model is available:\")\n",
        "    logger.info(f\"  ollama pull {CONFIG['llm_model']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDataset loaded:\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1m  train:  781 examples\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1m    dev:  260 examples\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1m   test:  241 examples\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mPrepared 781 articles from training set for RAG knowledge base\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mSample article:\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import data loading functions\n",
        "from src.data import load_processed_data\n",
        "\n",
        "# Load VLSP 2018 NER dataset\n",
        "data_splits = load_processed_data()\n",
        "\n",
        "logger.info(f\"Dataset loaded:\")\n",
        "for split_name, split_data in data_splits.items():\n",
        "    logger.info(f\"  {split_name:>5}: {len(split_data):>4} examples\")\n",
        "\n",
        "# Use train split for RAG knowledge base\n",
        "train_data = data_splits['train']\n",
        "dev_data = data_splits['dev']\n",
        "test_data = data_splits['test']\n",
        "\n",
        "# Create articles from train data for RAG\n",
        "training_articles = []\n",
        "for item in train_data:\n",
        "    article = {\n",
        "        \"id\": str(item['id']),\n",
        "        \"text\": item['text'],\n",
        "        \"source\": \"vlsp_2018_train\",\n",
        "        \"domain\": item.get('topic', 'general'),\n",
        "        \"ground_truth\": item['ground_truth']  # Store ground truth for reference\n",
        "    }\n",
        "    training_articles.append(article)\n",
        "\n",
        "logger.info(f\"Prepared {len(training_articles)} articles from training set for RAG knowledge base\")\n",
        "\n",
        "# Display sample\n",
        "sample = training_articles[0]\n",
        "logger.info(f\"Sample article:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Knowledge Base Ingestion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define `KnowledgeBaseManager`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeBaseManager:\n",
        "    \"\"\"Production-grade knowledge base ingestion manager with crash recovery\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 vector_store: Chroma,\n",
        "                 chunker: VietnameseDocumentChunker,\n",
        "                 batch_size: int = 5,\n",
        "                 max_retries: int = 3,\n",
        "                 retry_delay: float = 2.0):\n",
        "        self.vector_store = vector_store\n",
        "        self.chunker = chunker\n",
        "        self.batch_size = batch_size\n",
        "        self.max_retries = max_retries\n",
        "        self.retry_delay = retry_delay\n",
        "        \n",
        "    def check_collection_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Check current collection status\"\"\"\n",
        "        try:\n",
        "            collection = self.vector_store._collection\n",
        "            count = collection.count()\n",
        "            return {\n",
        "                \"exists\": True,\n",
        "                \"count\": count,\n",
        "                \"collection_name\": collection.name\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to check collection status: {e}\")\n",
        "            return {\"exists\": False, \"count\": 0}\n",
        "    \n",
        "    def clear_collection(self, confirm: bool = False):\n",
        "        \"\"\"Clear all documents from collection\"\"\"\n",
        "        if not confirm:\n",
        "            raise ValueError(\"Must set confirm=True to clear collection\")\n",
        "        \n",
        "        try:\n",
        "            collection = self.vector_store._collection\n",
        "            current_count = collection.count()\n",
        "            \n",
        "            if current_count > 0:\n",
        "                collection.delete(where={})\n",
        "                logger.info(f\"Cleared {current_count} documents from collection\")\n",
        "            else:\n",
        "                logger.info(\"Collection already empty\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to clear collection: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def ingest_articles(self, \n",
        "                       articles: List[Dict],\n",
        "                       strategy: str = \"full\",\n",
        "                       ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ingest articles with automatic crash recovery\n",
        "        \n",
        "        Args:\n",
        "            articles: List of article dicts\n",
        "            strategy: 'chunked' or 'full'\n",
        "        \n",
        "        Returns:\n",
        "            Ingestion statistics\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        start_time = time.time()\n",
        "        documents = []\n",
        "        stats = {\n",
        "            \"articles_processed\": 0,\n",
        "            \"chunks_created\": 0,\n",
        "            \"documents_indexed\": 0,\n",
        "            \"failed\": 0,\n",
        "            \"failed_ids\": [],\n",
        "            \"strategy\": strategy,\n",
        "        }\n",
        "        \n",
        "        # Step 1: Prepare documents\n",
        "        logger.info(\"Step 1/2: Preparing documents...\")\n",
        "        \n",
        "        for article in tqdm(articles, desc=\"Preparing documents\"):\n",
        "            try:\n",
        "                if strategy == \"chunked\":\n",
        "                    chunks = self.chunker.chunk_with_metadata(\n",
        "                        text=article[\"text\"],\n",
        "                        doc_id=article[\"id\"],\n",
        "                        metadata={\n",
        "                            \"source\": article[\"source\"],\n",
        "                            \"domain\": article[\"domain\"]\n",
        "                        }\n",
        "                    )\n",
        "                    \n",
        "                    for chunk in chunks:\n",
        "                        doc = Document(\n",
        "                            page_content=chunk[\"text\"],\n",
        "                            metadata={\n",
        "                                \"doc_id\": chunk[\"doc_id\"],\n",
        "                                \"chunk_index\": chunk[\"chunk_index\"],\n",
        "                                \"char_count\": chunk[\"char_count\"],\n",
        "                                **chunk[\"metadata\"]\n",
        "                            }\n",
        "                        )\n",
        "                        documents.append(doc)\n",
        "                        stats[\"chunks_created\"] += 1\n",
        "                \n",
        "                elif strategy == \"full\":\n",
        "                    # Max 1000 chars per chunk\n",
        "                    text = article[\"text\"][:1000]\n",
        "                    doc = Document(\n",
        "                        page_content=text,\n",
        "                        metadata={\n",
        "                            \"doc_id\": article[\"id\"],\n",
        "                            \"source\": article[\"source\"],\n",
        "                            \"domain\": article[\"domain\"],\n",
        "                            \"full_article\": True\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "                    stats[\"chunks_created\"] += 1\n",
        "                \n",
        "                stats[\"articles_processed\"] += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to prepare article {article['id']}: {e}\")\n",
        "                stats[\"failed\"] += 1\n",
        "                stats[\"failed_ids\"].append(article[\"id\"])\n",
        "        \n",
        "        \n",
        "        if len(documents) == 0:\n",
        "            logger.warning(\"No documents to ingest!\")\n",
        "            return stats\n",
        "        \n",
        "        # Index with retry\n",
        "        logger.info(\"Step 2/2: Indexing with crash recovery...\")\n",
        "        \n",
        "        consecutive_failures = 0\n",
        "        max_consecutive_failures = 3\n",
        "        \n",
        "        for i in tqdm(range(0, len(documents), self.batch_size), desc=\"Indexing\"):\n",
        "            batch = documents[i:i+self.batch_size]\n",
        "            batch_success = False\n",
        "            \n",
        "            for attempt in range(self.max_retries):\n",
        "                try:\n",
        "                    # Batch embedding\n",
        "                    self.vector_store.add_documents(batch)\n",
        "                    stats[\"documents_indexed\"] += len(batch)\n",
        "                    batch_success = True\n",
        "                    consecutive_failures = 0  # Reset on success\n",
        "                    \n",
        "                    # Conservative delay for stability\n",
        "                    time.sleep(1.0)  # 1 second between batches\n",
        "                    break\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    error_msg = str(e)\n",
        "                    \n",
        "                    if attempt < self.max_retries - 1:\n",
        "                        wait_time = self.retry_delay * (2.0 ** attempt)  # Aggressive backoff\n",
        "                        logger.warning(f\"Batch {i//self.batch_size} failed (attempt {attempt+1}), waiting {wait_time:.1f}s\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        logger.error(f\"Batch {i//self.batch_size} failed permanently\")\n",
        "                        stats[\"failed\"] += len(batch)\n",
        "                        for doc in batch:\n",
        "                            stats[\"failed_ids\"].append(doc.metadata.get(\"doc_id\", \"unknown\"))\n",
        "                        consecutive_failures += 1\n",
        "            \n",
        "        # Final stats\n",
        "        elapsed_time = time.time() - start_time\n",
        "        stats[\"elapsed_time\"] = elapsed_time\n",
        "        stats[\"docs_per_second\"] = stats[\"documents_indexed\"] / elapsed_time if elapsed_time > 0 else 0\n",
        "        \n",
        "        \n",
        "        return stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize `KnowledgeBaseManager`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.967\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[32m\u001b[1mKnowledgeBaseManager initialized with auto-recovery\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mCurrent collection status: {'exists': True, 'count': 1674, 'collection_name': 'ner_kb'}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "kb_manager = KnowledgeBaseManager(\n",
        "    vector_store=vector_store,\n",
        "    chunker=chunker,\n",
        "    batch_size=2,\n",
        "    max_retries=5,\n",
        "    retry_delay=3.0\n",
        ")\n",
        "\n",
        "status = kb_manager.check_collection_status()\n",
        "logger.success(\"KnowledgeBaseManager initialized with auto-recovery\")\n",
        "logger.info(f\"Current collection status: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.990\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[32m\u001b[1mEmbedding model initialized: nomic-embed-text:latest\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:19.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1m   Vector dimension: 768\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "embedder = OllamaEmbeddings(\n",
        "    model=CONFIG[\"embedding_model\"],\n",
        "    base_url=CONFIG[\"ollama_host\"]\n",
        ")\n",
        "\n",
        "# Test embedding\n",
        "try:\n",
        "    test_embedding = embedder.embed_query(\"Kiểm tra\")\n",
        "    logger.success(f\"Embedding model initialized: {CONFIG['embedding_model']}\")\n",
        "    logger.info(f\"   Vector dimension: {len(test_embedding)}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to initialize embedding model: {e}\")\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ingest training data into knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.996\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mWarning: Collection already contains 1674 documents!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Check if we should ingest\n",
        "status = kb_manager.check_collection_status()\n",
        "current_count = status['count']\n",
        "\n",
        "\n",
        "if current_count > 0:\n",
        "    logger.warning(f\"Warning: Collection already contains {current_count} documents!\")\n",
        "    \n",
        "else:\n",
        "    logger.info(\"Collection is empty, starting ingestion...\")\n",
        "    \n",
        "    # Using chunked strategy\n",
        "    INGESTION_STRATEGY = \"chunked\"  # Preserves ALL content (0% data loss)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Run ingestion\n",
        "    ingestion_stats = kb_manager.ingest_articles(\n",
        "        articles=training_articles,\n",
        "        strategy=INGESTION_STRATEGY\n",
        "    )\n",
        "    \n",
        "    # Print results\n",
        "    logger.success(\"INGESTION COMPLETE\")\n",
        "    logger.info(f\"Results:\")\n",
        "    logger.info(f\"  Strategy: {ingestion_stats['strategy']}\")\n",
        "    logger.info(f\"  Articles processed: {ingestion_stats['articles_processed']}/{len(training_articles)}\")\n",
        "    logger.info(f\"  Documents created: {ingestion_stats['chunks_created']}\")\n",
        "    logger.info(f\"  Documents indexed: {ingestion_stats['documents_indexed']}\")\n",
        "    logger.error(f\"  Failed: {ingestion_stats['failed']}\")\n",
        "    \n",
        "    # Check success rate\n",
        "    if ingestion_stats['chunks_created'] > 0:\n",
        "        success_rate = (ingestion_stats['documents_indexed'] / ingestion_stats['chunks_created']) * 100\n",
        "        logger.success(f\"  Success rate: {success_rate:.1f}%\")\n",
        "        \n",
        "        if success_rate < 90:\n",
        "            logger.warning(f\"Warning: Low success rate!\")\n",
        "        else:\n",
        "            logger.success(f\"Excellent success rate!\")\n",
        "    \n",
        "    # Verify final count\n",
        "    final_status = kb_manager.check_collection_status()\n",
        "    logger.info(f\"Final collection count: {final_status['count']} documents\")\n",
        "    \n",
        "    logger.info(\"=\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test NER Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:19.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mInput Text: Lê Văn E là giám đốc của Công ty Cổ phần Phần mềm ABC. Anh ấy sinh năm 1985 tại \n",
            "Sài Gòn. Công ty ABC có văn phòng chính tại đường Nguyễn Hữu Cảnh, Hồ Chí Minh.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Test input\n",
        "test_input = \"\"\"Lê Văn E là giám đốc của Công ty Cổ phần Phần mềm ABC. Anh ấy sinh năm 1985 tại \n",
        "Sài Gòn. Công ty ABC có văn phòng chính tại đường Nguyễn Hữu Cảnh, Hồ Chí Minh.\"\"\"\n",
        "\n",
        "logger.info(f\"Input Text: {test_input}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m{\n",
            "  \"person\": [\n",
            "    \"Lê Văn E\",\n",
            "    \"Anh ấy\"\n",
            "  ],\n",
            "  \"organizations\": [\n",
            "    \"Công ty Cổ phần Phần mềm ABC\"\n",
            "  ],\n",
            "  \"address\": [\n",
            "    \"Sài Gòn\",\n",
            "    \"đường Nguyễn Hữu Cảnh, Hồ Chí Minh\"\n",
            "  ]\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTiming: 1.647s\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Extract without RAG (baseline)\n",
        "baseline_result = ner_extractor.extract_with_rag(\n",
        "    text=test_input,\n",
        "    use_rag=False,\n",
        "    return_context=True\n",
        ")\n",
        "\n",
        "if baseline_result[\"error\"]:\n",
        "    logger.error(f\"Error: {baseline_result['error']}\")\n",
        "else:\n",
        "    logger.info(json.dumps(baseline_result[\"entities\"], ensure_ascii=False, indent=2))\n",
        "    if baseline_result[\"retrieval_info\"]:\n",
        "        logger.info(f\"Timing: {baseline_result['retrieval_info']['total_time']:.3f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:21.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1m----------------------------------------\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m{\n",
            "  \"person\": [\n",
            "    \"Lê Văn E\"\n",
            "  ],\n",
            "  \"organizations\": [\n",
            "    \"Công ty Cổ phần Phần mềm ABC\"\n",
            "  ],\n",
            "  \"address\": [\n",
            "    \"đường Nguyễn Hữu Cảnh, Hồ Chí Minh\"\n",
            "  ]\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mRetrieval Info:\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1m  Chunks retrieved: 4\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m  Retrieval time: 0.013s\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m  LLM time: 2.176s\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1m  Total time: 2.190s\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Extract with RAG\n",
        "logger.info(\"-\"*40)\n",
        "\n",
        "rag_result = ner_extractor.extract_with_rag(\n",
        "    text=test_input,\n",
        "    use_rag=True,\n",
        "    return_context=True\n",
        ")\n",
        "\n",
        "if rag_result[\"error\"]:\n",
        "    logger.error(f\"Error: {rag_result['error']}\")\n",
        "else:\n",
        "    logger.info(json.dumps(rag_result[\"entities\"], ensure_ascii=False, indent=2))\n",
        "    if rag_result[\"retrieval_info\"]:\n",
        "        info = rag_result[\"retrieval_info\"]\n",
        "        logger.info(f\"Retrieval Info:\")\n",
        "        logger.info(f\"  Chunks retrieved: {info['num_chunks']}\")\n",
        "        logger.info(f\"  Retrieval time: {info['retrieval_time']:.3f}s\")\n",
        "        logger.info(f\"  LLM time: {info['llm_time']:.3f}s\")\n",
        "        logger.info(f\"  Total time: {info['total_time']:.3f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation & Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:23:23.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPrepared validation set: 30 examples from dev split\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m  person         :  140 (36.5%)\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m  organizations  :   92 (24.0%)\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m  address        :  152 (39.6%)\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mSample validation example:\u001b[0m\n",
            "\u001b[32m2025-12-14 10:23:23.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m  Entities: 8 total\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Prepare validation set from dev data\n",
        "# Use a subset for faster evaluation\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Sample validation examples from dev set\n",
        "val_size = 30  # Use 30 examples for validation\n",
        "val_indices = random.sample(range(len(dev_data)), min(val_size, len(dev_data)))\n",
        "\n",
        "validation_examples = []\n",
        "for idx in val_indices:\n",
        "    item = dev_data[idx]\n",
        "    validation_examples.append({\n",
        "        \"id\": item['id'],\n",
        "        \"text\": item['text'],\n",
        "        \"ground_truth\": item['ground_truth']\n",
        "    })\n",
        "\n",
        "logger.info(f\"Prepared validation set: {len(validation_examples)} examples from dev split\")\n",
        "total_entities = sum(sum(len(v) for v in ex['ground_truth'].values()) for ex in validation_examples)\n",
        "\n",
        "# Show entity distribution\n",
        "entity_counts = {'person': 0, 'organizations': 0, 'address': 0}\n",
        "for ex in validation_examples:\n",
        "    for entity_type in entity_counts:\n",
        "        entity_counts[entity_type] += len(ex['ground_truth'].get(entity_type, []))\n",
        "\n",
        "for entity_type, count in entity_counts.items():\n",
        "    logger.info(f\"  {entity_type:15s}: {count:4d} ({count/total_entities*100:.1f}%)\")\n",
        "\n",
        "# Display sample\n",
        "sample = validation_examples[0]\n",
        "logger.info(f\"Sample validation example:\")\n",
        "logger.info(f\"  Entities: {sum(len(v) for v in sample['ground_truth'].values())} total\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate both methods on validation set\n",
        "\n",
        "baseline_predictions = []\n",
        "rag_predictions = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i, example in enumerate(tqdm(validation_examples, desc=\"Evaluating\")):\n",
        "    # Baseline (without RAG)\n",
        "    base = ner_extractor.extract_with_rag(example[\"text\"], use_rag=False)\n",
        "    baseline_predictions.append(base[\"entities\"])\n",
        "    \n",
        "    # With RAG\n",
        "    rag = ner_extractor.extract_with_rag(example[\"text\"], use_rag=True)\n",
        "    rag_predictions.append(rag[\"entities\"])\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "ground_truths = [ex[\"ground_truth\"] for ex in validation_examples]\n",
        "\n",
        "logger.success(f\"Evaluation complete in {elapsed:.1f}s ({elapsed/len(validation_examples):.1f}s per example)\")\n",
        "\n",
        "# Calculate metrics\n",
        "logger.info(\"Calculating metrics...\")\n",
        "baseline_metrics = calculate_accuracy(baseline_predictions, ground_truths)\n",
        "rag_metrics = calculate_accuracy(rag_predictions, ground_truths)\n",
        "\n",
        "logger.info(\"RESULTS\")\n",
        "\n",
        "logger.info(f\"  Exact Match Accuracy: {baseline_metrics['accuracy']:.1%}\")\n",
        "logger.info(f\"  Overall Precision: {baseline_metrics['overall_entity_metrics']['precision']:.1%}\")\n",
        "logger.info(f\"  Overall Recall: {baseline_metrics['overall_entity_metrics']['recall']:.1%}\")\n",
        "logger.info(f\"  Overall F1: {baseline_metrics['overall_entity_metrics']['f1']:.1%}\")\n",
        "\n",
        "logger.info(\"  Per-entity-type metrics:\")\n",
        "for entity_type, metrics in baseline_metrics['per_entity_type'].items():\n",
        "    logger.info(f\"    {entity_type:15s}: P={metrics['precision']:.1%}, R={metrics['recall']:.1%}, F1={metrics['f1']:.1%}\")\n",
        "\n",
        "logger.info(f\"  Exact Match Accuracy: {rag_metrics['accuracy']:.1%}\")\n",
        "logger.info(f\"  Overall Precision: {rag_metrics['overall_entity_metrics']['precision']:.1%}\")\n",
        "logger.info(f\"  Overall Recall: {rag_metrics['overall_entity_metrics']['recall']:.1%}\")\n",
        "logger.info(f\"  Overall F1: {rag_metrics['overall_entity_metrics']['f1']:.1%}\")\n",
        "\n",
        "logger.info(\"  Per-entity-type metrics:\")\n",
        "for entity_type, metrics in rag_metrics['per_entity_type'].items():\n",
        "    logger.info(f\"    {entity_type:15s}: P={metrics['precision']:.1%}, R={metrics['recall']:.1%}, F1={metrics['f1']:.1%}\")\n",
        "\n",
        "logger.info(\"3. IMPROVEMENT:\")\n",
        "rag_f1 = baseline_metrics['overall_entity_metrics']['f1']\n",
        "baseline_f1 = rag_metrics['overall_entity_metrics']['f1']\n",
        "overall_improvement = (rag_f1 - baseline_f1) * 100\n",
        "\n",
        "logger.info(f\"  Overall F1 improvement: {overall_improvement:+.1f}%\")\n",
        "logger.info(f\"  Exact Match improvement: {(rag_metrics['accuracy'] - baseline_metrics['accuracy']) * 100:+.1f}%\")\n",
        "\n",
        "logger.info(\"  Per-entity-type F1 improvement:\")\n",
        "for entity_type in baseline_metrics['per_entity_type'].keys():\n",
        "    baseline_et_f1 = baseline_metrics['per_entity_type'][entity_type]['f1']\n",
        "    rag_et_f1 = rag_metrics['per_entity_type'][entity_type]['f1']\n",
        "    improvement = (rag_et_f1 - baseline_et_f1) * 100\n",
        "    logger.info(f\"    {entity_type:15s}: {improvement:+.1f}%\")\n",
        "\n",
        "# Print comparison table\n",
        "\n",
        "comparison_dict = {\n",
        "    'Baseline (No RAG)': baseline_metrics,\n",
        "    'With RAG': rag_metrics\n",
        "}\n",
        "\n",
        "print_comparison_table(comparison_dict)\n",
        "\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'config': CONFIG,\n",
        "    'validation_size': len(validation_examples),\n",
        "    'baseline_metrics': baseline_metrics,\n",
        "    'rag_metrics': rag_metrics,\n",
        "    'improvement': {\n",
        "        'overall_f1': overall_improvement,\n",
        "        'exact_match': (rag_metrics['accuracy'] - baseline_metrics['accuracy']) * 100\n",
        "    },\n",
        "    'elapsed_time': elapsed\n",
        "}\n",
        "\n",
        "logger.info(f\" Evaluation results ready\")\n",
        "logger.info(f\"  Baseline F1: {baseline_f1:.1%}\")\n",
        "logger.info(f\"  RAG F1: {rag_f1:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Performance Profiling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitor RAG pipeline performance\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.timings = defaultdict(list)\n",
        "    \n",
        "    def record(self, operation: str, duration: float):\n",
        "        self.timings[operation].append(duration)\n",
        "    \n",
        "    def report(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate performance report\"\"\"\n",
        "        \n",
        "        rows = []\n",
        "        for operation, times in self.timings.items():\n",
        "            rows.append({\n",
        "                \"Operation\": operation,\n",
        "                \"Count\": len(times),\n",
        "                \"Mean (ms)\": np.mean(times) * 1000,\n",
        "                \"Median (ms)\": np.median(times) * 1000,\n",
        "                \"Min (ms)\": np.min(times) * 1000,\n",
        "                \"Max (ms)\": np.max(times) * 1000,\n",
        "                \"Std (ms)\": np.std(times) * 1000\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "# Profile extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 10:34:22.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPerformance Profiling (10 iterations):\u001b[0m\n",
            "\u001b[32m2025-12-14 10:34:59.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m           Operation  Count    Mean (ms)  Median (ms)     Min (ms)     Max (ms)   Std (ms)\n",
            "0  Baseline (No RAG)     10  1494.510007  1480.045795  1462.295532  1589.990854  35.897936\n",
            "1           With RAG     10  2188.977551  2188.072205  2183.880329  2197.580814   3.407714\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "monitor = PerformanceMonitor()\n",
        "\n",
        "logger.info(\"Performance Profiling (10 iterations):\")\n",
        "\n",
        "for i in range(10):\n",
        "    # Baseline\n",
        "    start = time.time()\n",
        "    ner_extractor.extract_with_rag(test_input, use_rag=False)\n",
        "    baseline_time = time.time() - start\n",
        "    monitor.record(\"Baseline (No RAG)\", baseline_time)\n",
        "    \n",
        "    # RAG\n",
        "    start = time.time()\n",
        "    ner_extractor.extract_with_rag(test_input, use_rag=True)\n",
        "    rag_time = time.time() - start\n",
        "    monitor.record(\"With RAG\", rag_time)\n",
        "\n",
        "logger.info(monitor.report().to_string())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "a2a",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
