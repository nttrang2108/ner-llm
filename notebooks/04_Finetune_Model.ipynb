{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Fine-tuning NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainerCallback,\n",
        "    EvalPrediction,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import custom data loader\n",
        "from src.data import load_processed_data\n",
        "\n",
        "# Import prompt builder\n",
        "from src.prompt import build_prompt\n",
        "\n",
        "# Import evaluation utilities\n",
        "from src.utils.evaluation import parse_ner_response\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Enhanced determinism for production\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "\n",
        "**Security Note**: Never hardcode tokens in notebooks. Use environment variables or secure credential management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:00 - WARNING - HF_TOKEN environment variable not set. If using a gated model, authentication will fail.\n",
            "2025-12-14 06:29:00 - INFO - Configuration initialized successfully\n",
            "2025-12-14 06:29:00 - INFO - Model: mistralai/Mistral-7B-Instruct-v0.3\n",
            "2025-12-14 06:29:00 - INFO - Output directory: checkpoints/mistral-7B-Instruct-v0.3-ner-qlora\n",
            "2025-12-14 06:29:00 - INFO - GPU available: True\n",
            "2025-12-14 06:29:00 - INFO - GPU device: NVIDIA GeForce RTX 4060 Ti\n",
            "2025-12-14 06:29:00 - INFO - GPU memory: 16.71 GB\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Training configuration parameters.\"\"\"\n",
        "    \n",
        "    # Model configuration\n",
        "    # Option 1: Ministral-3-14B (requires transformers >= 4.47.0)\n",
        "    # model_name: str = \"mistralai/Ministral-3-8B-Instruct-2512\"\n",
        "    \n",
        "    # Option 2: Mistral-7B (works with older transformers, still excellent for NER)\n",
        "    model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    \n",
        "    max_length: int = 1536\n",
        "    \n",
        "    # Data configuration\n",
        "    data_dir: Path = Path(\"../data/vlps_2018_ner/processed\")\n",
        "    train_file: str = \"train.json\"\n",
        "    val_ratio: float = 0.1\n",
        "    seed: int = 42\n",
        "    \n",
        "    # QLoRA configuration\n",
        "    lora_r: int = 64  # Increased for better capacity\n",
        "    lora_alpha: int = 128  # 2x lora_r is recommended\n",
        "    lora_dropout: float = 0.05  # Lower dropout for better convergence\n",
        "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # All attention projections\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP layers for better performance\n",
        "    ])\n",
        "    \n",
        "    # Training configuration\n",
        "    output_dir: str = \"./checkpoints/mistral-7B-Instruct-v0.3-ner-qlora\"\n",
        "    num_train_epochs: int = 10\n",
        "    per_device_train_batch_size: int = 1\n",
        "    per_device_eval_batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 8  # Effective batch size = 8\n",
        "    eval_accumulation_steps: int = 8  # Match training accumulation for consistency\n",
        "    learning_rate: float = 2e-4  # Standard for QLoRA\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_ratio: float = 0.03\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "    max_grad_norm: float = 1.0  # Standard gradient clipping (was 0.3, too aggressive)\n",
        "    optim: str = \"paged_adamw_8bit\"\n",
        "    \n",
        "    # Logging and checkpointing\n",
        "    logging_steps: int = 10\n",
        "    eval_steps: int = 100\n",
        "    save_steps: int = 100\n",
        "    save_total_limit: int = 3\n",
        "    \n",
        "    # Hardware configuration\n",
        "    bf16: bool = True  # Use bfloat16 for better stability\n",
        "    use_4bit: bool = True\n",
        "    bnb_4bit_compute_dtype: str = \"bfloat16\"\n",
        "    bnb_4bit_quant_type: str = \"nf4\"  # Normal Float 4-bit\n",
        "    use_nested_quant: bool = True  # Double quantization for memory savings\n",
        "    \n",
        "    # HuggingFace token (read from environment)\n",
        "    hf_token: Optional[str] = field(default_factory=lambda: os.getenv(\"HF_TOKEN\"))\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization.\"\"\"\n",
        "        self.data_dir = Path(self.data_dir)\n",
        "        self.output_dir = Path(self.output_dir)\n",
        "        \n",
        "        if not self.data_dir.exists():\n",
        "            raise ValueError(f\"Data directory does not exist: {self.data_dir}\")\n",
        "        \n",
        "        train_path = self.data_dir / self.train_file\n",
        "        if not train_path.exists():\n",
        "            raise ValueError(f\"Training file does not exist: {train_path}\")\n",
        "        \n",
        "        # Create output directory\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Warn if HF token is not set for gated models\n",
        "        if self.hf_token is None:\n",
        "            logger.warning(\n",
        "                \"HF_TOKEN environment variable not set. \"\n",
        "                \"If using a gated model, authentication will fail.\"\n",
        "            )\n",
        "        \n",
        "        # Validate batch configuration\n",
        "        if self.per_device_eval_batch_size < self.per_device_train_batch_size:\n",
        "            logger.warning(\n",
        "                f\"Eval batch size ({self.per_device_eval_batch_size}) is smaller than \"\n",
        "                f\"train batch size ({self.per_device_train_batch_size}). Consider increasing it.\"\n",
        "            )\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "logger.info(f\"Configuration initialized successfully\")\n",
        "logger.info(f\"Model: {config.model_name}\")\n",
        "logger.info(f\"Output directory: {config.output_dir}\")\n",
        "logger.info(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    logger.info(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Processing\n",
        "\n",
        "Define the NER instruction prompt and data loading utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:00 - INFO - Loading data using src.data loader...\n",
            "2025-12-14 06:29:00 - INFO - Loaded 260 examples\n",
            "2025-12-14 06:29:00 - INFO - Split: 234 train, 26 validation\n",
            "2025-12-14 06:29:00 - WARNING - Validation set very small (26 examples). Consider increasing val_ratio for more reliable evaluation.\n",
            "2025-12-14 06:29:00 - INFO - \n",
            "=== Sample Training Example ===\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "You are a Vietnamese Named Entity Recognition (NER) expert. Extract named entities from the given text and classify them into three categories:\n",
            "- person: Names of people\n",
            "- organizations: Names of organizations, companies, institutions\n",
            "- address: Location names, addresses\n",
            "\n",
            "Return your answer as a JSON object with these three keys. Each value should be a list of strings. If a category has no entities, return an empty list. Do not invent entities that are not present in the text.\n",
            "\n",
            "...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def build_training_prompt(example: Dict, include_response: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Build a formatted prompt for training.\n",
        "    Uses build_prompt() from src.prompt for the base prompt,\n",
        "    then adds ground truth response for training.\n",
        "    \n",
        "    Args:\n",
        "        example: Dictionary containing 'text' and optionally 'ground_truth'\n",
        "        include_response: Whether to include the response (for training)\n",
        "    \n",
        "    Returns:\n",
        "        Formatted prompt string with optional response\n",
        "    \"\"\"\n",
        "    # Use the imported build_prompt for consistency\n",
        "    prompt = build_prompt(example['text'])\n",
        "    \n",
        "    if include_response and 'ground_truth' in example:\n",
        "        gt = example.get('ground_truth', {}) or {}\n",
        "        response = {\n",
        "            \"person\": gt.get(\"person\", []),\n",
        "            \"organizations\": gt.get(\"organizations\", []),\n",
        "            \"address\": gt.get(\"address\", []),\n",
        "        }\n",
        "        prompt += f\"\\n{json.dumps(response, ensure_ascii=False, indent=2)}\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def split_train_data(\n",
        "    data: List[Dict], \n",
        "    val_ratio: float = 0.1, \n",
        "    seed: int = 42\n",
        ") -> Tuple[List[Dict], List[Dict]]:\n",
        "    \"\"\"\n",
        "    Split data into train and validation sets.\n",
        "    \n",
        "    Args:\n",
        "        data: List of examples\n",
        "        val_ratio: Validation set ratio\n",
        "        seed: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (train_data, val_data)\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Shuffle data\n",
        "    shuffled_data = data.copy()\n",
        "    random.shuffle(shuffled_data)\n",
        "    \n",
        "    # Split\n",
        "    val_size = max(1, int(len(data) * val_ratio))\n",
        "    val_data = shuffled_data[:val_size]\n",
        "    train_data = shuffled_data[val_size:]\n",
        "    \n",
        "    # Warn if validation set is too small\n",
        "    if val_size < 50:\n",
        "        logger.warning(\n",
        "            f\"Validation set very small ({val_size} examples). \"\n",
        "            f\"Consider increasing val_ratio for more reliable evaluation.\"\n",
        "        )\n",
        "    \n",
        "    return train_data, val_data\n",
        "\n",
        "\n",
        "# Load and split data\n",
        "logger.info(\"Loading data using src.data loader...\")\n",
        "all_data = load_processed_data(\"data/vlps_2018_ner/processed/train.json\")\n",
        "logger.info(f\"Loaded {len(all_data)} examples\")\n",
        "\n",
        "train_examples, val_examples = split_train_data(all_data, val_ratio=config.val_ratio, seed=42)\n",
        "logger.info(f\"Split: {len(train_examples)} train, {len(val_examples)} validation\")\n",
        "\n",
        "# Display sample\n",
        "logger.info(\"\\n=== Sample Training Example ===\")\n",
        "sample_prompt = build_training_prompt(train_examples[0], include_response=True)\n",
        "print(sample_prompt[:500] + \"...\\n\" if len(sample_prompt) > 500 else sample_prompt + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Setup\n",
        "\n",
        "Load the base model with 4-bit quantization and prepare it for QLoRA training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=config.use_4bit,\n",
        "    bnb_4bit_quant_type=config.bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, config.bnb_4bit_compute_dtype),\n",
        "    bnb_4bit_use_double_quant=config.use_nested_quant,\n",
        ")\n",
        "\n",
        "logger.info(\"Loading tokenizer...\")\n",
        "\n",
        "# Ministral models may have tokenizer backend issues\n",
        "# Try multiple methods with graceful fallbacks\n",
        "tokenizer = None\n",
        "methods_tried = []\n",
        "\n",
        "# Method 1: Try MistralTokenizerFast (recommended for Ministral)\n",
        "try:\n",
        "    from transformers import MistralTokenizerFast\n",
        "    logger.info(\"Method 1: Attempting MistralTokenizerFast...\")\n",
        "    tokenizer = MistralTokenizerFast.from_pretrained(\n",
        "        config.model_name,\n",
        "        token=config.hf_token,\n",
        "    )\n",
        "    logger.info(\"✓ Successfully loaded MistralTokenizerFast\")\n",
        "except Exception as e:\n",
        "    methods_tried.append(f\"MistralTokenizerFast: {str(e)[:50]}\")\n",
        "    logger.warning(f\"MistralTokenizerFast failed: {e}\")\n",
        "\n",
        "# Method 2: Try slow MistralTokenizer\n",
        "if tokenizer is None:\n",
        "    try:\n",
        "        from transformers import MistralTokenizer\n",
        "        logger.info(\"Method 2: Attempting MistralTokenizer (slow)...\")\n",
        "        tokenizer = MistralTokenizer.from_pretrained(\n",
        "            config.model_name,\n",
        "            token=config.hf_token,\n",
        "        )\n",
        "        logger.info(\"✓ Successfully loaded MistralTokenizer\")\n",
        "    except Exception as e:\n",
        "        methods_tried.append(f\"MistralTokenizer: {str(e)[:50]}\")\n",
        "        logger.warning(f\"MistralTokenizer failed: {e}\")\n",
        "\n",
        "# Method 3: Try PreTrainedTokenizerFast directly with Mistral-7B vocab\n",
        "if tokenizer is None:\n",
        "    try:\n",
        "        logger.info(\"Method 3: Attempting standard Mistral-7B tokenizer...\")\n",
        "        # Use a standard Mistral model's tokenizer (compatible with Ministral)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "            token=config.hf_token,\n",
        "        )\n",
        "        logger.info(\"✓ Successfully loaded Mistral-7B tokenizer (compatible with Ministral)\")\n",
        "        logger.info(\"Note: Using Mistral-7B tokenizer for Ministral-3-14B (they share the same vocabulary)\")\n",
        "    except Exception as e:\n",
        "        methods_tried.append(f\"Mistral-7B tokenizer: {str(e)[:50]}\")\n",
        "        logger.warning(f\"Mistral-7B tokenizer failed: {e}\")\n",
        "\n",
        "# Method 4: AutoTokenizer without trust_remote_code\n",
        "if tokenizer is None:\n",
        "    try:\n",
        "        logger.info(\"Method 4: Attempting AutoTokenizer (no trust_remote_code)...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            config.model_name,\n",
        "            token=config.hf_token,\n",
        "            trust_remote_code=False,\n",
        "            use_fast=False,\n",
        "        )\n",
        "        logger.info(\"✓ Successfully loaded AutoTokenizer\")\n",
        "    except Exception as e:\n",
        "        methods_tried.append(f\"AutoTokenizer: {str(e)[:50]}\")\n",
        "        logger.error(f\"AutoTokenizer failed: {e}\")\n",
        "\n",
        "if tokenizer is None:\n",
        "    error_msg = \"All tokenizer loading methods failed:\\n\" + \"\\n\".join(f\"  - {m}\" for m in methods_tried)\n",
        "    logger.error(error_msg)\n",
        "    raise RuntimeError(error_msg)\n",
        "\n",
        "# Set padding token (required for batching)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "logger.info(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
        "logger.info(f\"Padding token: {tokenizer.pad_token}\")\n",
        "\n",
        "logger.info(\"Loading base model with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    token=config.hf_token,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(\n",
        "    model,\n",
        "    use_gradient_checkpointing=True,  # Save memory at cost of speed\n",
        ")\n",
        "\n",
        "logger.info(\"Model loaded successfully\")\n",
        "logger.info(f\"Model dtype: {model.dtype}\")\n",
        "logger.info(f\"Model device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 Verify Target Modules (Critical Check)\n",
        "\n",
        "Before applying LoRA, verify that the target modules exist in the Ministral architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:15 - INFO - Verifying LoRA target modules in model architecture...\n",
            "2025-12-14 06:29:15 - INFO - Configured target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "2025-12-14 06:29:15 - INFO - \n",
            "Found projection/gate modules in model: ['act_fn', 'down_proj', 'gate_proj', 'k_proj', 'mlp', 'o_proj', 'post_attention_layernorm', 'q_proj', 'up_proj', 'v_proj']\n",
            "2025-12-14 06:29:15 - INFO - \n",
            "✅ All target modules verified successfully!\n",
            "2025-12-14 06:29:15 - INFO - \n",
            "Modules that will have LoRA adapters: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "2025-12-14 06:29:15 - INFO - Number of adapter layers: 7\n"
          ]
        }
      ],
      "source": [
        "# Verify target modules exist in the model\n",
        "logger.info(\"Verifying LoRA target modules in model architecture...\")\n",
        "logger.info(f\"Configured target modules: {config.lora_target_modules}\")\n",
        "\n",
        "# Find all projection and gate modules\n",
        "projection_modules = set()\n",
        "for name, module in model.named_modules():\n",
        "    if any(keyword in name.lower() for keyword in ['proj', 'gate', 'mlp', 'attention']):\n",
        "        # Extract the layer-independent name\n",
        "        parts = name.split('.')\n",
        "        if len(parts) > 0:\n",
        "            module_name = parts[-1]\n",
        "            projection_modules.add(module_name)\n",
        "\n",
        "logger.info(f\"\\nFound projection/gate modules in model: {sorted(projection_modules)}\")\n",
        "\n",
        "# Check if configured modules exist\n",
        "missing_modules = []\n",
        "for target_module in config.lora_target_modules:\n",
        "    if target_module not in projection_modules:\n",
        "        missing_modules.append(target_module)\n",
        "\n",
        "if missing_modules:\n",
        "    logger.warning(f\"\\n  WARNING: These target modules don't exist in the model: {missing_modules}\")\n",
        "    logger.warning(\"LoRA will NOT be applied to these modules!\")\n",
        "    logger.warning(\"Consider updating config.lora_target_modules to match actual model architecture.\")\n",
        "else:\n",
        "    logger.info(\"\\n All target modules verified successfully!\")\n",
        "\n",
        "# Log actual modules that will be targeted\n",
        "actual_targets = [m for m in config.lora_target_modules if m in projection_modules]\n",
        "logger.info(f\"\\nModules that will have LoRA adapters: {actual_targets}\")\n",
        "logger.info(f\"Number of adapter layers: {len(actual_targets)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LoRA Configuration\n",
        "\n",
        "Apply Low-Rank Adaptation (LoRA) to enable efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:15 - INFO - Applying LoRA adapters...\n",
            "2025-12-14 06:29:17 - INFO - \n",
            "============================================================\n",
            "2025-12-14 06:29:17 - INFO - Trainable parameters: 167,772,160\n",
            "2025-12-14 06:29:17 - INFO - Total parameters: 3,926,134,784\n",
            "2025-12-14 06:29:17 - INFO - Trainable %: 4.27%\n",
            "2025-12-14 06:29:17 - INFO - ============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 167,772,160 || all params: 7,415,795,712 || trainable%: 2.2624\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    target_modules=config.lora_target_modules,\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        ")\n",
        "\n",
        "logger.info(\"Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_percent = 100 * trainable_params / all_params\n",
        "\n",
        "logger.info(f\"\\n{'='*60}\")\n",
        "logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "logger.info(f\"Total parameters: {all_params:,}\")\n",
        "logger.info(f\"Trainable %: {trainable_percent:.2f}%\")\n",
        "logger.info(f\"{'='*60}\\n\")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:17 - INFO - Created dataset with 234 examples\n",
            "2025-12-14 06:29:17 - INFO - Created dataset with 26 examples\n",
            "2025-12-14 06:29:17 - INFO - Training dataset size: 234\n",
            "2025-12-14 06:29:17 - INFO - Validation dataset size: 26\n",
            "2025-12-14 06:29:17 - INFO - \n",
            "============================================================\n",
            "2025-12-14 06:29:17 - INFO - Effective batch size: 8\n",
            "2025-12-14 06:29:17 - INFO - Total training steps: 290\n",
            "2025-12-14 06:29:17 - INFO - Evaluations per epoch: ~0\n",
            "2025-12-14 06:29:17 - INFO - ============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create tokenized datasets\n",
        "train_dataset = NERDataset(train_examples, tokenizer, config.max_length)\n",
        "val_dataset = NERDataset(val_examples, tokenizer, config.max_length)\n",
        "\n",
        "logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
        "logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# Calculate training statistics\n",
        "effective_batch_size = config.per_device_train_batch_size * config.gradient_accumulation_steps\n",
        "total_steps = (len(train_dataset) // effective_batch_size) * config.num_train_epochs\n",
        "eval_steps_per_epoch = len(train_dataset) // effective_batch_size // (config.eval_steps + 1)\n",
        "\n",
        "logger.info(f\"\\n{'='*60}\")\n",
        "logger.info(f\"Effective batch size: {effective_batch_size}\")\n",
        "logger.info(f\"Total training steps: {total_steps}\")\n",
        "logger.info(f\"Evaluations per epoch: ~{eval_steps_per_epoch}\")\n",
        "logger.info(f\"{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.5 Evaluation Metrics with Seqeval\n",
        "\n",
        "Add proper NER evaluation metrics: precision, recall, and F1 per entity type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.6 Inference Helper Function\n",
        "\n",
        "Define the prediction function that will be used for evaluation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:17 - INFO - Inference helper function defined\n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    text: str, \n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.0,  # Use greedy decoding for structured output\n",
        "    top_p: float = 0.9,\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate NER predictions for a given text with robust JSON parsing.\n",
        "    \n",
        "    Args:\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Tokenizer\n",
        "        text: Input text\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature (0.0 for deterministic)\n",
        "        top_p: Nucleus sampling parameter\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with extracted entities\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if not text or not isinstance(text, str):\n",
        "        logger.error(\"Invalid input text\")\n",
        "        return {\"error\": \"Invalid input\", \"person\": [], \"organizations\": [], \"address\": []}\n",
        "    \n",
        "    # Build prompt without response\n",
        "    prompt = build_training_prompt({\"text\": text}, include_response=False)\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=temperature > 0,\n",
        "            top_p=top_p if temperature > 0 else None,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract response\n",
        "    if \"### Response:\" in generated_text:\n",
        "        response_text = generated_text.split(\"### Response:\")[-1].strip()\n",
        "        \n",
        "        # Use robust parsing with fallback to regex\n",
        "        entities = parse_entities_from_json(response_text)\n",
        "        return entities\n",
        "    \n",
        "    logger.warning(\"No response marker found in generated text\")\n",
        "    return {\"error\": \"No response generated\", \"person\": [], \"organizations\": [], \"address\": []}\n",
        "\n",
        "\n",
        "logger.info(\"Inference helper function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:17 - INFO - Evaluation metrics functions defined\n",
            "2025-12-14 06:29:17 - INFO - Note: Full entity-level F1/precision/recall will be computed post-training\n"
          ]
        }
      ],
      "source": [
        "def parse_entities_from_json(json_str: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Parse entities from JSON string with fallback to regex extraction.\n",
        "    \n",
        "    This is a wrapper around the utility function for backward compatibility.\n",
        "    Uses robust parsing that handles:\n",
        "    - Markdown code blocks (```)\n",
        "    - Extra text before/after JSON\n",
        "    - Key variations (organizations vs organization)\n",
        "    \n",
        "    Args:\n",
        "        json_str: JSON string containing entities (may include markdown)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with entity categories and lists\n",
        "    \"\"\"\n",
        "    # Use the robust utility function\n",
        "    return parse_ner_response(json_str)\n",
        "\n",
        "\n",
        "def compute_ner_metrics(predictions: List[Dict], references: List[Dict]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute NER metrics using seqeval format.\n",
        "    \n",
        "    Args:\n",
        "        predictions: List of predicted entity dictionaries\n",
        "        references: List of ground truth entity dictionaries\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with precision, recall, F1 scores\n",
        "    \"\"\"\n",
        "    # Convert to seqeval format: list of lists of entity labels\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    \n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_entities = []\n",
        "        true_entities = []\n",
        "        \n",
        "        # Aggregate all entity types for this example\n",
        "        for entity_type in [\"person\", \"organizations\", \"address\"]:\n",
        "            pred_ents = pred.get(entity_type, [])\n",
        "            true_ents = ref.get(entity_type, [])\n",
        "            \n",
        "            # Add labels in BIO format for seqeval\n",
        "            pred_entities.extend([f\"B-{entity_type.upper()}\" for _ in pred_ents])\n",
        "            true_entities.extend([f\"B-{entity_type.upper()}\" for _ in true_ents])\n",
        "        \n",
        "        # If empty, add \"O\" (outside)\n",
        "        if not pred_entities:\n",
        "            pred_entities = [\"O\"]\n",
        "        if not true_entities:\n",
        "            true_entities = [\"O\"]\n",
        "        \n",
        "        pred_labels.append(pred_entities)\n",
        "        true_labels.append(true_entities)\n",
        "    \n",
        "    # Compute metrics\n",
        "    try:\n",
        "        precision = precision_score(true_labels, pred_labels)\n",
        "        recall = recall_score(true_labels, pred_labels)\n",
        "        f1 = f1_score(true_labels, pred_labels)\n",
        "        \n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to compute seqeval metrics: {e}\")\n",
        "        return {\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"f1\": 0.0,\n",
        "        }\n",
        "\n",
        "\n",
        "def compute_metrics_callback(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Custom compute_metrics function for Trainer.\n",
        "    \n",
        "    Note: For causal LM training, detailed entity extraction during training\n",
        "    is expensive. This is a placeholder that returns loss-based metrics.\n",
        "    Use the separate evaluation script for full NER metrics.\n",
        "    \n",
        "    Args:\n",
        "        eval_pred: EvalPrediction object with predictions and label_ids\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with metrics\n",
        "    \"\"\"\n",
        "    # For now, we rely on eval_loss computed by Trainer\n",
        "    # Full entity-level evaluation should be done separately post-training\n",
        "    return {}\n",
        "\n",
        "\n",
        "logger.info(\"Evaluation metrics functions defined\")\n",
        "logger.info(\"Note: Full entity-level F1/precision/recall will be computed post-training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:17 - INFO - Training arguments configured successfully\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # Output configuration\n",
        "    output_dir=str(config.output_dir),\n",
        "    overwrite_output_dir=False,\n",
        "    \n",
        "    # Training configuration\n",
        "    num_train_epochs=config.num_train_epochs,\n",
        "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # Optimization configuration\n",
        "    learning_rate=config.learning_rate,\n",
        "    weight_decay=config.weight_decay,\n",
        "    optim=config.optim,\n",
        "    lr_scheduler_type=config.lr_scheduler_type,\n",
        "    warmup_ratio=config.warmup_ratio,\n",
        "    max_grad_norm=config.max_grad_norm,  # Gradient clipping (now 1.0, standard value)\n",
        "    \n",
        "    # Precision configuration\n",
        "    bf16=config.bf16,\n",
        "    fp16=False,\n",
        "    \n",
        "    # Logging configuration\n",
        "    logging_dir=str(config.output_dir / \"logs\"),\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=config.logging_steps,\n",
        "    logging_first_step=True,\n",
        "    \n",
        "    # Evaluation configuration (use eval_strategy instead of evaluation_strategy for newer transformers)\n",
        "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
        "    eval_steps=config.eval_steps,\n",
        "    eval_accumulation_steps=config.eval_accumulation_steps,  # Consistent with training\n",
        "    \n",
        "    # Checkpointing configuration\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=config.save_steps,\n",
        "    save_total_limit=config.save_total_limit,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    \n",
        "    # Other configuration\n",
        "    report_to=\"tensorboard\",\n",
        "    disable_tqdm=False,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    seed=config.seed,\n",
        ")\n",
        "\n",
        "logger.info(\"Training arguments configured successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Custom Callbacks\n",
        "\n",
        "Add custom callbacks for better monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log GPU memory usage.\"\"\"\n",
        "    \n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % args.logging_steps == 0 and torch.cuda.is_available():\n",
        "            memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            logger.info(\n",
        "                f\"GPU Memory - Allocated: {memory_allocated:.2f} GB, \"\n",
        "                f\"Reserved: {memory_reserved:.2f} GB\"\n",
        "            )\n",
        "\n",
        "\n",
        "class SampleGenerationCallback(TrainerCallback):\n",
        "    \"\"\"Callback to generate sample predictions during training.\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer, sample_text: str, eval_every: int = 500):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_text = sample_text\n",
        "        self.eval_every = eval_every\n",
        "    \n",
        "    def on_step_end(self, args, state, control, model, **kwargs):\n",
        "        if state.global_step % self.eval_every == 0 and state.global_step > 0:\n",
        "            logger.info(\"\\n\" + \"=\"*60)\n",
        "            logger.info(\"Generating sample prediction...\")\n",
        "            \n",
        "            # Create prompt without response\n",
        "            prompt = build_training_prompt({\"text\": self.sample_text}, include_response=False)\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            \n",
        "            # Generate\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=256,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                )\n",
        "            \n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Extract only the response part\n",
        "            if \"### Response:\" in generated_text:\n",
        "                response = generated_text.split(\"### Response:\")[-1].strip()\n",
        "                logger.info(f\"Generated Response:\\n{response[:500]}\")\n",
        "            \n",
        "            model.train()\n",
        "            logger.info(\"=\"*60 + \"\\n\")\n",
        "\n",
        "\n",
        "# Prepare sample for generation callback\n",
        "sample_text = val_examples[0]['text'] if val_examples else train_examples[0]['text']\n",
        "\n",
        "callbacks = [\n",
        "    MemoryCallback(),\n",
        "    SampleGenerationCallback(tokenizer, sample_text, eval_every=500),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:17 - INFO - Trainer initialized successfully\n",
            "2025-12-14 06:29:17 - INFO - Ready to start training with 234 examples\n"
          ]
        }
      ],
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "logger.info(\"Trainer initialized successfully\")\n",
        "logger.info(f\"Ready to start training with {len(train_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train the Model\n",
        "\n",
        "Start the training process. Monitor the progress through logs and TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:29:17 - INFO - \n",
            "============================================================\n",
            "2025-12-14 06:29:17 - INFO - Starting training...\n",
            "2025-12-14 06:29:17 - INFO - ============================================================\n",
            "\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='167' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [167/300 1:18:03 < 1:02:54, 0.04 it/s, Epoch 5.55/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.148500</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.045400</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 06:34:05 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 06:38:55 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 06:43:21 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 06:48:10 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 06:52:59 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 06:57:26 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:02:14 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:07:03 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:11:30 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:16:19 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:21:36 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:26:03 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:30:52 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:35:40 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:40:07 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:44:56 - INFO - GPU Memory - Allocated: 5.38 GB, Reserved: 8.44 GB\n",
            "2025-12-14 07:48:03 - WARNING - Training interrupted by user\n"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"Starting training...\")\n",
        "logger.info(\"=\"*60 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    \n",
        "    # Log training results\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"Training completed successfully!\")\n",
        "    logger.info(f\"Training loss: {train_result.training_loss:.4f}\")\n",
        "    logger.info(f\"Training steps: {train_result.global_step}\")\n",
        "    logger.info(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # Save metrics\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    logger.warning(\"Training interrupted by user\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training failed with error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 07:48:08 - INFO - Evaluating model on validation set...\n",
            "2025-12-14 07:48:36 - INFO - \n",
            "============================================================\n",
            "2025-12-14 07:48:36 - INFO - Evaluation Results:\n",
            "2025-12-14 07:48:36 - INFO - eval_loss: nan\n",
            "2025-12-14 07:48:36 - INFO - ============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  eval_loss = nan\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set\n",
        "logger.info(\"Evaluating model on validation set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"Evaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    logger.info(f\"{key}: {value:.4f}\")\n",
        "logger.info(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Save evaluation results\n",
        "trainer.log_metrics(\"eval\", eval_results)\n",
        "trainer.save_metrics(\"eval\", eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 07:48:36 - INFO - Saving LoRA adapters to checkpoints/mistral-7B-Instruct-v0.3-ner-qlora/final-adapters\n",
            "2025-12-14 07:48:37 - INFO - LoRA adapters saved successfully\n",
            "2025-12-14 07:48:37 - INFO - Adapter size: 675.56 MB\n"
          ]
        }
      ],
      "source": [
        "# Save LoRA adapters\n",
        "adapter_dir = config.output_dir / \"final-adapters\"\n",
        "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logger.info(f\"Saving LoRA adapters to {adapter_dir}\")\n",
        "model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "\n",
        "logger.info(\"LoRA adapters saved successfully\")\n",
        "logger.info(f\"Adapter size: {sum(f.stat().st_size for f in adapter_dir.rglob('*') if f.is_file()) / 1e6:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Test Inference\n",
        "\n",
        "Test the fine-tuned model with sample inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 07:48:37 - INFO - \n",
            "============================================================\n",
            "2025-12-14 07:48:37 - INFO - Testing inference on sample examples...\n",
            "2025-12-14 07:48:37 - INFO - ============================================================\n",
            "\n",
            "2025-12-14 07:48:37 - INFO - \n",
            "Example 1:\n",
            "2025-12-14 07:48:37 - INFO - Input text: Công nghệ Hawk-Eye hay còn được biết đến là mắt thần sẽ được BTC giải ATP Next Gen Finals sử dụng cho tất cả các tình huống diễn ra trên sân, qua đó loại bỏ hoàn toàn các vị trọng tài dây. Giải sẽ đượ...\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "2025-12-14 07:48:59 - INFO - \n",
            "Prediction:\n",
            "2025-12-14 07:48:59 - INFO - \n",
            "Ground Truth:\n",
            "2025-12-14 07:48:59 - INFO - \n",
            "------------------------------------------------------------\n",
            "2025-12-14 07:48:59 - INFO - \n",
            "Example 2:\n",
            "2025-12-14 07:48:59 - INFO - Input text: NDĐT – Với mục tiêu bảo đảm khai thác hiệu quả, nâng cao chất lượng cơ sở vật chất phục vụ giáo dục thể chất, trường THPT chuyên Hà Nội -Amsterdam đã báo cáo với Sở Giáo dục và Đào tạo (GD-ĐT) Hà Nội ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"person\": [],\n",
            "  \"organizations\": [\n",
            "    \"Hawk-Eye\"\n",
            "  ],\n",
            "  \"address\": [\n",
            "    \"Milan\"\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  \"person\": [],\n",
            "  \"organizations\": [],\n",
            "  \"address\": [\n",
            "    \"Milan\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 07:49:39 - INFO - \n",
            "Prediction:\n",
            "2025-12-14 07:49:39 - INFO - \n",
            "Ground Truth:\n",
            "2025-12-14 07:49:39 - INFO - \n",
            "------------------------------------------------------------\n",
            "2025-12-14 07:49:39 - INFO - \n",
            "Example 3:\n",
            "2025-12-14 07:49:39 - INFO - Input text: Làm giả giấy tờ, hợp đồng mua bán thẻ cào để tạo lòng tin, nữ giám đốc lừa đảo nhiều người chiếm đoạt hàng trăm tỉ đồng. Ngày 22-9, TAND TP HCM mở phiên tòa sơ thẩm xét xử bị cáo Lã Thị Thanh (44 tuổi...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"person\": [\n",
            "    \"Nguyễn Đình Vinh\",\n",
            "    \"MINH PHƯƠNG\"\n",
            "  ],\n",
            "  \"organizations\": [],\n",
            "  \"address\": [\n",
            "    \"Hà Nội\",\n",
            "    \"Amsterdam\",\n",
            "    \"TP Hà Nội\"\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  \"person\": [\n",
            "    \"Nguyễn Đình Vinh\",\n",
            "    \"MINH PHƯƠNG\"\n",
            "  ],\n",
            "  \"organizations\": [\n",
            "    \"NDĐT\"\n",
            "  ],\n",
            "  \"address\": [\n",
            "    \"Amsterdam\",\n",
            "    \"thành phố Hà Nội\",\n",
            "    \"quận Cầu Giấy\",\n",
            "    \"TP. Hà Nội\",\n",
            "    \"Hà Nội -Amsterdam\",\n",
            "    \"Hà Nội\",\n",
            "    \"Hà Nội Amsterdam\",\n",
            "    \"TP Hà Nội\",\n",
            "    \"Hà Nội - Amsterdam\",\n",
            "    \"phường Trung Hòa\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 07:50:07 - INFO - \n",
            "Prediction:\n",
            "2025-12-14 07:50:07 - INFO - \n",
            "Ground Truth:\n",
            "2025-12-14 07:50:07 - INFO - \n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"person\": [\n",
            "    \"Lã Thị Thanh\",\n",
            "    \"Nguyễn Quang Đức\",\n",
            "    \"Đức\",\n",
            "    \"Quang Anh\",\n",
            "    \"Nguyễn Quang Đức\",\n",
            "    \"Thanh\",\n",
            "    \"Quoc Chiến\"\n",
            "  ],\n",
            "  \"organizations\": [\n",
            "    \"Công ty Petechland Jsc\",\n",
            "    \"Công ty TMDV Quang Anh\",\n",
            "    \"Viettel\",\n",
            "    \"Công ty TNHH Khánh Linh\"\n",
            "  ],\n",
            "  \"address\": [\n",
            "    \"TP HCM\",\n",
            "    \"quận 1\",\n",
            "    \"đường Trần Hưng Đạo\",\n",
            "    \"phường Phạm Ngũ Lão\",\n",
            "    \"Ninh Bình\",\n",
            "    \"thành phố Hà Nội\"\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  \"person\": [\n",
            "    \"Lã Thị Thanh\",\n",
            "    \"Thanh\",\n",
            "    \"Nguyễn Quang Đức\",\n",
            "    \"Đức\",\n",
            "    \"Quốc Chiến\",\n",
            "    \"Khánh Linh\"\n",
            "  ],\n",
            "  \"organizations\": [\n",
            "    \"Công ty Petechland Jsc\",\n",
            "    \"Công ty TMDV Quang Anh\",\n",
            "    \"công ty Quang Anh\",\n",
            "    \"Viettel\"\n",
            "  ],\n",
            "  \"address\": [\n",
            "    \"quận 1\",\n",
            "    \"TP HCM\",\n",
            "    \"đường Trần Hưng Đạo\",\n",
            "    \"phường Phạm Ngũ Lão\",\n",
            "    \"Ninh Bình\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Note: generate_predictions() function was already defined in section 8.6\n",
        "# It's available for use here\n",
        "\n",
        "# Test with validation examples\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"Testing inference on sample examples...\")\n",
        "logger.info(\"=\"*60 + \"\\n\")\n",
        "\n",
        "num_samples = min(3, len(val_examples))\n",
        "for i in range(num_samples):\n",
        "    example = val_examples[i]\n",
        "    \n",
        "    logger.info(f\"\\nExample {i+1}:\")\n",
        "    logger.info(f\"Input text: {example['text'][:200]}...\")\n",
        "    \n",
        "    # Generate prediction\n",
        "    prediction = generate_predictions(model, tokenizer, example['text'])\n",
        "    \n",
        "    logger.info(f\"\\nPrediction:\")\n",
        "    print(json.dumps(prediction, ensure_ascii=False, indent=2))\n",
        "    \n",
        "    if 'ground_truth' in example:\n",
        "        logger.info(f\"\\nGround Truth:\")\n",
        "        print(json.dumps(example['ground_truth'], ensure_ascii=False, indent=2))\n",
        "    \n",
        "    logger.info(\"\\n\" + \"-\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "a2a",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
